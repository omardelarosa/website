---
title: Readings | Augmented Reality Art & Applications | Week 9
date: 1554319615663
createdAt: 1554319615663
publishedAt: 1554319615663
slug: readings-augmented-reality-art-and-applications
tags: ["process", "reading_response", "w09", "citylens"]
---

In the process of researching my City As Cite project, I've continued to cross reference academic ideas from _Computers Helping People With Special Needs_ with less academic, pop-cultural thinking on how to combine public spaces with technology. For my "pop cultural" angle this week, I found some interesting ideas in an old article from Wired [_Augmented Reality is Transforming Museums_](https://www.wired.com/story/augmented-reality-art-museums/).

The article deals with how various digital artists have used augmented reality to "enhance" the content of museums, often without the cooperation of museums themselves. One example they cited in the article is artist Jeff Koons' attempt to impose a 3D "sculpture" of a baloon dog in folks' geo-tagged photos:

![](https://techcrunch.com/wp-content/uploads/2017/10/screen-shot-2017-10-08-at-1-19-20-pm.png?w=1390&crop=1)

In most cases, though, the artists use ARKit-powered apps to add different, often meta-layers of context to galleries as a source of commentary. For example the project [AR(T)](https://www.hackingtheheist.com/) augments gallery content.

While certainly playful and interesting, after reading I asked myself if there were ways to turn ideas these playful exhibits into "environmental annotations" for people with some kind of special need. For instance, if ideas from concepts such as AR(T) can be combined with the, albeit dated ideas, from _ENABLE â€“ A View on User's Needs_, an article from _Computers Helping People With Special Needs_. Their philosophy is as follows:

> The ENABLE project builds on the concepts of user centred design [2]. The goal is to build a system that meets user's needs across Europe in the best possible way, to be able to help a large number of people to manage and maintain their daily life as independently as possible.

So one idea that stuck out to me was perhaps using a few features from Vuforia's AR API such as [Image Targets](https://library.vuforia.com/articles/Training/Image-Target-Guide) to place objects in the real world the, when viewed with certain mobile applications, trigger additional context to appear. For example, perhaps for the hearing impaired they could aim a phone app at some sticker or image on an object in the real world the depends on auditory data and have a visual illustration pop up that gives them some timely information about the object. Thus creating a possible implementation for "environmental annotations".

![](https://vuforialibrarycontent.vuforia.com/Images/devGuide_ImageTargets.jpg)
